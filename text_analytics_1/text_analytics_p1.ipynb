{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794243c0",
   "metadata": {},
   "source": [
    "\n",
    "# Instructions:\n",
    "\n",
    "This notebook is forms the first part of your coursework assignment for Text Analytics in Spring 2025. You will need to read the instructions below and complete numbered tasks indicated by \"TASK n\". To complete the tasks, you will write code or explanations between the comments \"#WRITE YOUR ANSWER HERE\" and \"#END OF ANSWER\". For example:\n",
    "\n",
    "TASK 0: Complete the function below to output \"hello world\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5486ee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def demo_fun():\n",
    "    # WRITE YOUR ANSWER HERE\n",
    "    print(\"hello world\")\n",
    "    # END OF ANSWER\n",
    "\n",
    "### DO NOT MODIFY\n",
    "demo_fun()\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a422f8fe",
   "metadata": {},
   "source": [
    "There is also some code in the cell that should not be modified. This code saves your results to file in the correct format, which is necessary for us to be able to mark your answers. Before you submit your notebook, please make sure this code has not been modified, then restart your kernel, clear all cell outputs, run all of your code once again, then save the notebook. \n",
    "\n",
    "Please note:\n",
    "  * The notebook you upload must include all the saved cell output after running all cells.\n",
    "  * The notebook code must be complete so that it reproduces all your output when we run it. \n",
    "  * For this coursework, we recommend that you use your virtual environment that you created for the labs. The packages you need are: numpy, scipy, nltk, pytorch, transformers and datasets (from HuggingFace), pandas, matplotlib and scikit-learn. \n",
    "\n",
    "## Marking guidelines:\n",
    "1. This notebook is worth 32% of the marks for the Text Analytics assignment.\n",
    "1. The number of marks for each task is shown alongside the task.\n",
    "1. We will evaluate the output of your code after running it, and marks will be awarded based on how well the output matches the task's instructions. \n",
    "1. We will give partial marks for incomplete or partially correct answers. \n",
    "1. We do not give additional marks for code style or comments, but clear code will help us to understand what you have done so that we can award partial marks where necessary. \n",
    "1. Unless the task asks you to implement something from scratch, there is no penalty for using software libraries in your implementation.\n",
    "\n",
    "## Support:\n",
    "\n",
    "The main source of support will be during the lab sessions. The TAs and lecturers will help you with questions about the lectures, the code provided for you in this notebook, and general questions about the topics we cover. For the assessed tasks, they can only answer clarifying questions about what you have to do. Please email Edwin if you have any other queries edwin.simpson@bristol.ac.uk and/or post your query to the Teams channel for this unit.\n",
    "\n",
    "## Deadline:\n",
    "\n",
    "The notebook must be submitted along with the second notebook on Blackboard before **Monday 28th April at 13.00**. \n",
    "\n",
    "## Submission:\n",
    "\n",
    "For this part of the assignment, please zip up the folder containing this file and the 'outputs' directory, containing the output from this notebook as .csv files. Please name your notebook file like this:\n",
    "   * Name this notebook 'text_analytics_part1_\\<student number\\>.ipynb'. Replace '\\<student number\\>' with your student number, which consists only of digits beginning with '2'. \n",
    "   * We mark anonymously, so please don't include your name in the notebook.\n",
    "\n",
    "You can submit the file on Blackboard to the submission point \"Text Analytics Part 1 Notebook\". Remember that the assignment also has parts 2 and 3, described in the PDF file on Blackboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff04e2",
   "metadata": {},
   "source": [
    "# Setup: random seeds\n",
    "\n",
    "Each student will work with slightly different data splits and model weights, which will be determined by setting your 'random seed'. \n",
    "We will check that your results come from using your random seed. Please set the seed in the cell below by changing the value of 'my_student_number' to your own student number (not your username, the number you can see on eVision that contains only digits). \n",
    "\n",
    "Using the correct seed ensures that your results are reproducible when we rerun your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9924f508-49ae-46c0-85e3-19a30cb2a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "80bd735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)  # Python's built-in random module\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU (if available)\n",
    "    torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True  # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.benchmark = False  # Disable benchmark mode for reproducibility\n",
    "\n",
    "### SET YOUR SEED TO YOUR STUDENT NUMBER HERE\n",
    "my_student_number = 2411243\n",
    "set_seed(my_student_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877d086",
   "metadata": {},
   "source": [
    "# Setup: loading the data\n",
    "\n",
    "Let's make a folder to save the output of your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60bd9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbf072-5db9-487e-b495-568ed95839ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d5ed3a6-a810-4360-b226-7b642d446697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.2 kB ? eta -:--:--\n",
      "   -------------------- ------------------- 256.0/491.2 kB 7.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 491.2/491.2 kB 5.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.2 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 409.6/481.2 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 481.2/481.2 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 146.7/146.7 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.3 MB 5.2 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.5/25.3 MB 6.6 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.9/25.3 MB 7.5 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 1.3/25.3 MB 7.3 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.6/25.3 MB 7.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.0/25.3 MB 7.4 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.3/25.3 MB 7.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.7/25.3 MB 7.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.0/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.4/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.7/25.3 MB 7.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.1/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.4/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.8/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.1/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.5/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.8/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 6.2/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 7.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.8/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.1/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.5/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.7/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.0/25.3 MB 7.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.8/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.2/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.4/25.3 MB 7.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.9/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.1/25.3 MB 7.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.6/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.0/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.7/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.0/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.4/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.7/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.1/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.5/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.8/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.0/25.3 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.4/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 15.1/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.3/25.3 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.9/25.3 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.3/25.3 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.7/25.3 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.1/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.4/25.3 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.7/25.3 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 18.1/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.5/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.8/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.1/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.4/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.0/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.6/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.0/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.4/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.0/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.4/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.6/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.0/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.7/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.3/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.3 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "Successfully installed datasets-3.5.0 huggingface-hub-0.30.1 multiprocess-0.70.16 pyarrow-19.0.1 xxhash-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ee86cf-c118-4931-9e73-a9a665c05687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (19.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fc2c4",
   "metadata": {},
   "source": [
    "Now, let's load some more packages we will need later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "annoying-ethiopia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use HuggingFace's datasets library to access the Emotion dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e90e82-bbdb-4532-91b1-58229d4f1043",
   "metadata": {},
   "source": [
    "\n",
    "The dataset classifies paragraphs taken from corporate disclosures that discuss climate-related issues. It classifiers them into \"risk\" (0), \"neutral\" (1) or \"opportunity\" (2) representing the sentiment of the paragraph.\n",
    "\n",
    "First we need to load the data. The data is already split into train, validation and test. The _validation_ set (also called 'development' set or 'devset') can be used to compute performance of your model when tuning hyperparameters, optimising combinations of features, or looking at the errors your model makes before improving it. This allows you to hold out the test set (i.e., not to look at it at all when developing your method) to give a fair evaluation of the model and how well it generalises to new examples. This avoids tuning the model to specific examples in the test set. An alternative approach to validation is to not use a single fixed validation set, but instead use [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "efaf1096-acce-4226-a172-5357f49e91c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 800 instances loaded\n",
      "Development/validation dataset with 200 instances loaded\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# load the original training set from HuggingFace\n",
    "train_dataset = load_dataset(\n",
    "    \"climatebert/climate_sentiment\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# we're going to create a new validation set by splitting the data\n",
    "dataset_splits = train_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset_splits[\"train\"]\n",
    "val_dataset = dataset_splits[\"test\"]\n",
    "\n",
    "train_texts = np.array(train_dataset[\"text\"])\n",
    "val_texts = np.array(val_dataset[\"text\"])\n",
    "\n",
    "train_labels = np.array(train_dataset[\"label\"])\n",
    "val_labels = np.array(val_dataset[\"label\"])\n",
    "\n",
    "print(f\"Training dataset with {len(train_texts)} instances loaded\")\n",
    "print(f\"Development/validation dataset with {len(val_texts)} instances loaded\")\n",
    "\n",
    "### DO NOT MODIFY\n",
    "# save gold labels to file\n",
    "pd.DataFrame(val_labels).to_csv('./outputs/val_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38916c42",
   "metadata": {},
   "source": [
    "In this notebook, you're going to build three different classifiers for this dataset, then compare how they work, and analyse the results. We are going to start by implementing a naïve Bayes classifier from scratch. \n",
    "\n",
    "We are going to begin by initialising some useful variables and doing some very simple pre-processing using CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5fb3e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lenovo\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "def preprocess(train_texts):\n",
    "    vectorizer = CountVectorizer(ngram_range=(2,2), tokenizer=word_tokenize)\n",
    "    X = vectorizer.fit_transform(train_texts).toarray()\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    X_val = vectorizer.transform(val_texts).toarray()\n",
    "\n",
    "    return X, X_val, vectorizer, num_features\n",
    "\n",
    "X, X_val, vectorizer, num_features = preprocess(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1f7d1",
   "metadata": {},
   "source": [
    "## TASK 1.1a\n",
    "\n",
    "Complete the function below to compute the class priors, $p(y_n = c)$ for each class label $c$, where $y_n$ is the class label of document $n$. Do not use the Sklearn implementation to do this, but implement it yourself, e.g., using Numpy functions. The function must output the class priors as a list or Numpy array containing the probabilities. You do not need to apply any smoothing or regularisation.    (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79e7da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35375 0.40375 0.2425 ]\n"
     ]
    }
   ],
   "source": [
    "def compute_class_priors(texts, labels):\n",
    "    priors = np.zeros(num_classes)\n",
    "\n",
    "    ### WRITE YOUR ANSWER HERE\n",
    "    total_count = len(labels)\n",
    "    for i in range(num_classes):\n",
    "        priors[i] = np.sum(labels == i) / total_count\n",
    "    ### END OF ANSWER\n",
    "    return priors\n",
    "\n",
    "class_priors = compute_class_priors(train_texts, train_labels)\n",
    "print(class_priors)\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(class_priors).to_csv('./outputs/11a_class_priors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c63e",
   "metadata": {},
   "source": [
    "## TASK 1.1b\n",
    "\n",
    "Complete the function below to extract n-gram features from the text, then compute the liklihood $p(x_{ni} = w | y_n = c)$ that the $i$ th n-gram in document $n$ is $w$, given that the class of $n$ is $c$. Again, do not use the Sklearn implementation to do this, but implement it yourself, e.g., using Numpy functions. The function must output the likelihoods as a 2D Numpy array containing probabilities. You should apply smoothing by adding counts of +1 to the counts of each feature.  (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "91d8722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_likelihoods(X, labels):\n",
    "\n",
    "    likelihoods = np.ones((num_features, num_classes))  # a 2D numpy array where you can store the likelihoods. Note that all values are initialised to one.\n",
    "\n",
    "    ### WRITE YOUR ANSWER HERE\n",
    "    for c in range(num_classes):\n",
    "        # Select all rows that belong to class c\n",
    "        class_docs = X[labels == c]\n",
    "        # Sum all documents in the class by column to get the total frequency for each feature\n",
    "        class_feature_counts = class_docs.sum(axis=0)  # shape: (num_features,)\n",
    "        # Plus 1 (already initialized to 1), so just add the frequency here\n",
    "        likelihoods[:, c] += class_feature_counts\n",
    "        # The total number of features (smoothed) for each class is normalized\n",
    "        likelihoods[:, c] /= likelihoods[:, c].sum()\n",
    "    ### END OF ANSWER\n",
    "    return likelihoods\n",
    "\n",
    "likelihoods = compute_feature_likelihoods(X, train_labels)\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(likelihoods).to_csv('./outputs/11b_likelihoods.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a867a",
   "metadata": {},
   "source": [
    "Now, we are going to use the code in the next cell to compute the log probabilities of each class for each text in the validation set. This code will use the previous functions you implemented, compute_class_priors and compute_feature_likelihoods. The log probabilities will be stored in the 'predictions' array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "30194a65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.09830430e+00 -1.75665996e-04 -9.65983187e+00]\n",
      " [-1.33783274e+01 -5.75739441e+00 -3.16588727e-03]\n",
      " [-3.87865384e-09 -1.93677836e+01 -3.41478473e+01]\n",
      " [-2.14559093e+01 -2.67787941e-01 -1.44846770e+00]\n",
      " [-1.27494991e+01 -6.95415080e-04 -7.27553514e+00]\n",
      " [ 0.00000000e+00 -6.46803107e+01 -9.08665020e+01]\n",
      " [-1.36056900e+01 -1.35663862e-06 -1.59096110e+01]\n",
      " [-1.09406526e-01 -2.30920050e+00 -5.45067385e+00]\n",
      " [-1.62123297e+01 -2.46520472e-07 -1.56765332e+01]\n",
      " [-1.33019296e+01 -4.12109525e+00 -1.63615265e-02]\n",
      " [-3.70467945e+01 -1.24269420e+01 -4.00911529e-06]\n",
      " [-1.00563051e+01 -4.29152373e-05 -3.04286350e+01]\n",
      " [ 0.00000000e+00 -6.11452127e+01 -6.42534524e+01]\n",
      " [-9.28535441e-01 -5.04750216e-01 -6.71574838e+00]\n",
      " [-1.09427339e+00 -4.07676396e-01 -1.06747148e+01]\n",
      " [-5.19431721e-01 -1.88272385e+00 -1.37449835e+00]\n",
      " [ 0.00000000e+00 -3.98032181e+01 -3.95901461e+01]\n",
      " [-1.20236343e+01 -1.09426514e-05 -1.22177625e+01]\n",
      " [-2.57293708e+01 -7.70633689e+00 -4.50068011e-04]\n",
      " [ 0.00000000e+00 -3.88937993e+01 -5.40534766e+01]\n",
      " [-3.35570819e+00 -3.80666016e-02 -6.00494754e+00]\n",
      " [-3.87484031e+00 -4.63315472e-01 -1.04968431e+00]\n",
      " [-9.73434881e+00 -2.07806936e-01 -1.67356625e+00]\n",
      " [-3.74795169e+00 -2.42431745e-02 -7.86044446e+00]\n",
      " [-8.55455738e-02 -2.50791944e+00 -7.50341947e+00]\n",
      " [-1.38966481e+02 -1.31549403e+02  0.00000000e+00]\n",
      " [-2.27500616e+01 -1.14947078e+01 -1.01840297e-05]\n",
      " [-5.16660634e+01 -4.00253168e+01  0.00000000e+00]\n",
      " [-7.40863015e-09 -1.96214481e+01 -1.92418887e+01]\n",
      " [-1.89038187e+01 -6.16842044e-09 -3.44655163e+01]\n",
      " [-1.94855634e+01 -3.67697339e-09 -2.21961642e+01]\n",
      " [-5.68434189e-14 -3.03522389e+01 -3.83569057e+01]\n",
      " [-2.70752444e+00 -8.36450922e-02 -4.30205928e+00]\n",
      " [-3.59032879e+01  0.00000000e+00 -3.63706781e+01]\n",
      " [-6.56808259e+00 -1.41939342e-03 -1.11837118e+01]\n",
      " [-1.63922393e+01 -7.60223884e-08 -4.03723362e+01]\n",
      " [-1.93845474e+01 -1.79641355e+00 -1.81393339e-01]\n",
      " [-2.00377702e-05 -1.08235426e+01 -1.59984180e+01]\n",
      " [-2.20598732e+01 -4.05549372e-09 -1.93901657e+01]\n",
      " [-3.31023496e+00 -4.39227375e-02 -5.04142833e+00]\n",
      " [-2.76155785e-01 -1.42357621e+00 -7.69712597e+00]\n",
      " [-5.49665441e-03 -5.20637527e+00 -1.64927355e+01]\n",
      " [-2.97580819e+00 -5.23634273e-02 -1.15242159e+01]\n",
      " [-6.94452801e+00 -8.12441131e-01 -5.88313996e-01]\n",
      " [-2.24999549e+01 -2.29465513e-08 -1.75975001e+01]\n",
      " [ 0.00000000e+00 -4.95936718e+01 -5.40016365e+01]\n",
      " [-4.86690360e+00 -8.26384662e-03 -7.53767793e+00]\n",
      " [-2.17922549e+01 -1.43732981e+00 -2.71233098e-01]\n",
      " [ 0.00000000e+00 -3.14444671e+01 -3.57277984e+01]\n",
      " [-2.90720729e+01 -1.47304922e+01 -4.00524300e-07]\n",
      " [ 0.00000000e+00 -3.64667761e+01 -3.93833734e+01]\n",
      " [-1.06058867e+01 -2.75825830e-02 -3.60524123e+00]\n",
      " [-3.11724053e+01 -2.21689334e-11 -2.45317984e+01]\n",
      " [-1.68577320e+01 -8.32680200e-08 -1.71526278e+01]\n",
      " [-7.78288087e-03 -4.85971806e+00 -1.95326995e+01]\n",
      " [-1.44963587e+01 -7.93742788e-06 -1.18098220e+01]\n",
      " [-4.99521825e+01 -3.97792488e+01  0.00000000e+00]\n",
      " [-1.94217325e+01 -1.23971343e+00 -3.41740116e-01]\n",
      " [-1.16515621e+00 -7.74900103e-01 -1.48114913e+00]\n",
      " [-9.45446032e+00 -7.83433925e-05 -2.07669617e+01]\n",
      " [-6.62323403e+00 -1.26865704e+00 -3.32035498e-01]\n",
      " [-1.70928109e+01 -3.98073894e-08 -1.99921317e+01]\n",
      " [-9.31164722e+00 -4.50589252e-04 -7.92906719e+00]\n",
      " [-2.53745288e+01 -1.57796968e+01 -1.40279553e-07]\n",
      " [-4.20174946e-02 -3.38152932e+00 -4.94042346e+00]\n",
      " [-9.08649730e-06 -1.17884303e+01 -1.34136729e+01]\n",
      " [-3.07538174e+01 -4.54747351e-13 -2.85572608e+01]\n",
      " [-9.29037469e-09 -1.84945630e+01 -2.66850781e+01]\n",
      " [-2.26526972e+01 -1.45178092e-10 -3.38474046e+01]\n",
      " [-3.48436456e+00 -1.10122560e-01 -2.60907753e+00]\n",
      " [-1.71808908e+01 -8.21083716e-01 -5.79737507e-01]\n",
      " [-1.50020346e-04 -8.80481465e+00 -2.82507115e+01]\n",
      " [-1.15020958e+01 -1.01090839e-05 -2.26373628e+01]\n",
      " [-1.79335113e-04 -8.62662663e+00 -1.67979468e+01]\n",
      " [-2.52925929e-08 -1.74927527e+01 -3.19605364e+01]\n",
      " [-1.06464355e+01 -5.16518031e-05 -1.04881393e+01]\n",
      " [-2.69511561e+00 -6.99499248e-02 -1.06000078e+01]\n",
      " [-2.04636308e-12 -2.68908580e+01 -5.19176460e+01]\n",
      " [-8.61950609e+00 -1.80582430e-04 -1.79067969e+01]\n",
      " [-1.71822732e+01 -1.12334402e+01 -1.32590778e-05]\n",
      " [-3.69253104e-02 -4.34988502e+00 -3.75743290e+00]\n",
      " [-9.36562291e+00 -4.17162039e+00 -1.56344424e-02]\n",
      " [-1.11496291e-01 -2.24941007e+00 -1.00342976e+01]\n",
      " [-1.01881870e+01 -4.37661300e-04 -7.82416221e+00]\n",
      " [-4.30764464e+00 -1.35641430e-02 -1.18226093e+01]\n",
      " [-4.19563948e+01  0.00000000e+00 -4.67955536e+01]\n",
      " [-3.07921326e+00 -4.73864763e-02 -8.16009782e+00]\n",
      " [ 0.00000000e+00 -7.33006108e+01 -8.65345474e+01]\n",
      " [-1.88713669e+01 -1.95512746e-05 -1.08428058e+01]\n",
      " [-1.97617831e+01 -2.61962896e-09 -2.62301587e+01]\n",
      " [-1.29881842e+01 -2.28901047e-06 -2.01285507e+01]\n",
      " [-2.34114091e+01 -1.15006765e-06 -1.36757495e+01]\n",
      " [-1.97865343e+01 -1.50687342e+01 -2.88134402e-07]\n",
      " [-3.61791477e+01 -2.59524755e+01 -5.34328137e-12]\n",
      " [-1.44786501e+01 -1.40413754e+01 -1.31305796e-06]\n",
      " [-2.83082355e+01 -7.45217221e-11 -2.33264151e+01]\n",
      " [-6.42330633e-12 -2.57678577e+01 -3.73595590e+01]\n",
      " [-7.74656905e+00 -4.32407183e-04 -1.62148946e+01]\n",
      " [-5.04836659e+00 -6.74112747e-03 -8.11625178e+00]\n",
      " [ 0.00000000e+00 -1.13726852e+02 -1.25708223e+02]\n",
      " [-1.04739092e-06 -1.37692089e+01 -3.62424917e+01]\n",
      " [-9.31874749e+00 -2.31152228e+00 -1.04472010e-01]\n",
      " [ 0.00000000e+00 -7.53655083e+01 -8.99192921e+01]\n",
      " [-5.81975957e-03 -5.18852821e+00 -8.40993685e+00]\n",
      " [-3.10843707e+01 -4.92462016e+00 -7.29200769e-03]\n",
      " [-4.31082015e-07 -1.46646902e+01 -1.95244441e+01]\n",
      " [-2.21268421e+01 -1.51205793e+01 -2.71399642e-07]\n",
      " [-8.88970426e-05 -9.32848071e+00 -1.71409464e+01]\n",
      " [-6.38492860e-01 -7.52360416e-01 -7.32419952e+00]\n",
      " [-7.64490133e+01  0.00000000e+00 -4.11938578e+01]\n",
      " [ 0.00000000e+00 -3.81013598e+01 -5.32808150e+01]\n",
      " [-5.13390598e+00 -5.91116514e-03 -1.52723492e+01]\n",
      " [-1.14078563e+01 -5.37472741e+00 -4.65410267e-03]\n",
      " [-1.19505512e+00 -3.60522402e-01 -1.40704893e+01]\n",
      " [-1.37436255e+01 -1.07478468e-06 -2.21004340e+01]\n",
      " [-7.34361631e+00 -6.47593639e-04 -1.42064761e+01]\n",
      " [-3.86083662e+01 -1.07327587e-01 -2.28505343e+00]\n",
      " [-1.60271718e+00 -2.42553683e-01 -4.26657873e+00]\n",
      " [-1.26882377e-05 -1.15408761e+01 -1.27290406e+01]\n",
      " [-3.25536236e-01 -1.29247962e+00 -5.72268214e+00]\n",
      " [-2.35777874e-02 -4.06694213e+00 -5.08768144e+00]\n",
      " [-3.09994785e+01 -2.17375439e+01 -3.62661012e-10]\n",
      " [-3.18462515e+01 -4.47190358e-02 -3.12963221e+00]\n",
      " [-2.16287049e-07 -1.53466597e+01 -3.17432935e+01]\n",
      " [-3.89798390e-03 -5.54924668e+00 -1.84429958e+01]\n",
      " [-1.26572766e+01 -2.25614671e+00 -1.10659572e-01]\n",
      " [-1.03410447e+01 -3.22811363e-05 -2.41311341e+01]\n",
      " [-3.74729872e-05 -1.01919090e+01 -2.81960406e+01]\n",
      " [-4.11432224e-03 -5.49534468e+00 -1.73566950e+01]\n",
      " [-1.25335240e+01 -4.68665428e-06 -1.37359126e+01]\n",
      " [-3.69043689e+01 -5.32054401e-11 -2.36560100e+01]\n",
      " [-8.91263880e-08 -1.62332102e+01 -4.40184480e+01]\n",
      " [-1.36056001e+01 -1.15615245e+01 -1.07592559e-05]\n",
      " [-5.66219411e+00 -2.22953577e-01 -1.62773875e+00]\n",
      " [-1.34242080e-03 -8.13229722e+00 -6.86122684e+00]\n",
      " [-1.02188657e+01 -6.65807418e+00 -1.32096374e-03]\n",
      " [-3.81093977e-03 -5.57179170e+00 -1.73896125e+01]\n",
      " [-2.35443969e-02 -3.86826970e+00 -6.04280074e+00]\n",
      " [-2.69694749e+01 -2.51247911e-10 -2.21122073e+01]\n",
      " [-2.84645656e-05 -1.04668648e+01 -4.20891121e+01]\n",
      " [-6.81980475e+01  0.00000000e+00 -7.50778800e+01]\n",
      " [-2.09881107e-01 -1.68887320e+00 -5.38347914e+00]\n",
      " [-2.47461203e+00 -2.40461256e+00 -1.91761321e-01]\n",
      " [ 0.00000000e+00 -5.66954832e+01 -6.45672323e+01]\n",
      " [-1.68368956e-07 -1.55971341e+01 -2.61590489e+01]\n",
      " [-3.71344206e+00 -2.46967427e-02 -1.39683756e+01]\n",
      " [-1.15991308e+01 -2.09886819e-03 -6.17179143e+00]\n",
      " [-1.90409644e-01 -1.75252746e+00 -1.00253424e+01]\n",
      " [-4.32407887e-10 -2.15616557e+01 -4.31486787e+01]\n",
      " [-3.67779090e+01  0.00000000e+00 -3.51659551e+01]\n",
      " [-2.67668503e+01 -6.86796102e-05 -9.58609257e+00]\n",
      " [-1.80748873e+00 -1.50312992e+00 -4.88572581e-01]\n",
      " [-1.94203522e+01 -2.25522355e+00 -1.10764095e-01]\n",
      " [-3.45448586e+01 -9.32459443e-10 -2.07932213e+01]\n",
      " [-3.69805578e+01  0.00000000e+00 -4.18069894e+01]\n",
      " [-4.74844954e+01  0.00000000e+00 -4.66162885e+01]\n",
      " [-1.10620965e+01 -6.32740049e+00 -1.80399456e-03]\n",
      " [-3.65397464e+00 -8.48461163e-01 -6.05065223e-01]\n",
      " [-3.77976326e+00 -8.11508895e+00 -2.33987157e-02]\n",
      " [-1.04960746e+00 -4.59399200e-01 -4.00294249e+00]\n",
      " [ 0.00000000e+00 -5.13033777e+01 -6.47888427e+01]\n",
      " [ 0.00000000e+00 -6.62282934e+01 -9.11182387e+01]\n",
      " [-1.57685689e+01 -1.41839791e-07 -2.94279435e+01]\n",
      " [-1.51145589e-04 -8.81182763e+00 -1.30392155e+01]\n",
      " [-6.26090066e+00 -1.91263217e-03 -1.35693801e+01]\n",
      " [-1.14196177e-04 -9.07764984e+00 -3.86083797e+01]\n",
      " [-6.98561304e+00 -4.79942208e+00 -9.20180766e-03]\n",
      " [-3.05294762e-04 -8.16721304e+00 -1.07502388e+01]\n",
      " [-1.29722307e+01 -8.72895430e+00 -1.64169048e-04]\n",
      " [-9.09494702e-13 -2.79300926e+01 -2.95108657e+01]\n",
      " [-1.86818881e+01 -1.09649105e+01 -1.73060102e-05]\n",
      " [-1.82087422e+01 -7.08647576e-08 -1.66541698e+01]\n",
      " [-3.04960970e+01 -5.68434189e-14 -4.31864307e+01]\n",
      " [-4.74320462e+00 -3.12434608e-02 -3.81445444e+00]\n",
      " [-1.17665009e+01 -1.30373779e-03 -6.64914569e+00]\n",
      " [-2.31011654e-10 -2.21981192e+01 -2.68546346e+01]\n",
      " [ 0.00000000e+00 -6.28565213e+01 -6.70392428e+01]\n",
      " [-2.53045713e+01 -1.02886588e-11 -3.15782913e+01]\n",
      " [-1.07698961e+01 -1.57033090e+00 -2.33190571e-01]\n",
      " [-2.58989710e-01 -1.47780845e+00 -1.03522573e+01]\n",
      " [-3.96926447e-06 -1.25372931e+01 -1.47856711e+01]\n",
      " [-2.38597007e+01 -1.20831107e+01 -5.65426632e-06]\n",
      " [-7.72220717e+01  0.00000000e+00 -3.80531423e+01]\n",
      " [-1.84121320e+01 -1.11636135e+01 -1.41911021e-05]\n",
      " [-1.66901138e+01 -8.60364293e+00 -1.83509605e-04]\n",
      " [-2.37838264e+01 -1.98383532e-10 -2.26100509e+01]\n",
      " [-2.33232165e+01 -3.92583388e-09 -1.93747754e+01]\n",
      " [-3.00463267e+01 -6.85571422e-07 -1.41930136e+01]\n",
      " [-2.81771849e+01 -2.80853834e+01 -1.19371180e-12]\n",
      " [-7.30437932e-11 -2.33398722e+01 -4.02497937e+01]\n",
      " [-8.62933276e+00 -1.78874020e-04 -1.64175144e+01]\n",
      " [-1.94576737e+01 -1.56530393e+01 -1.62755612e-07]\n",
      " [-2.63949836e+01 -1.85772004e+01 -8.55459348e-09]\n",
      " [-3.50057667e+00 -2.13311827e-01 -1.82067850e+00]\n",
      " [-1.16529476e+00 -5.85048286e-01 -2.03186884e+00]\n",
      " [-1.40127226e+01 -8.21034178e-07 -2.47783586e+01]\n",
      " [-5.60871241e-05 -1.18090429e+01 -9.93088854e+00]\n",
      " [-2.62324175e+01 -2.51513331e-01 -1.50338155e+00]\n",
      " [ 0.00000000e+00 -4.20467540e+01 -6.01232015e+01]\n",
      " [-5.38751566e-01 -8.75813319e-01 -1.42388958e+01]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def NB_classify(class_priors, likelihoods, X_val):\n",
    "\n",
    "    predictions = np.zeros((X_val.shape[0], num_classes))  # an empty numpy array to store the predictions in\n",
    "\n",
    "    sum_of_log_likelihoods = X_val.dot(np.log(likelihoods))\n",
    "    log_joint_prob = sum_of_log_likelihoods + np.log(class_priors)[None, :]\n",
    "    for n, doc in enumerate(X_val):\n",
    "        predictions[n, :] = log_joint_prob[n]\n",
    "        predictions[n, :] -= logsumexp(predictions[n, :])\n",
    "    return predictions\n",
    "\n",
    "predictions = NB_classify(class_priors, likelihoods, X_val)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d91f5",
   "metadata": {},
   "source": [
    "Use the 'predictions' array above to compute and print the accuracy of the classifier on the validation set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c70cca90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(val_labels, np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322e627",
   "metadata": {},
   "source": [
    "## TASK 1.1c\n",
    "\n",
    "The simplicty of naïve Bayes means that we can quite easily interpret the model. In the code above, we used the functions you implemented, compute_feature_likelihoods and compute_class_priors, to train an NB classifier with our training set. Given this classifier, which are the five n-gram features that most strongly indicate that the document belongs to class 0? Store these features in the 'top_features' list below.    (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6369d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['climate change', ', and', 'of the', 'in the', 'to the']\n"
     ]
    }
   ],
   "source": [
    "top_features = []\n",
    "\n",
    "### WRITE YOUR ANSWER HERE\n",
    "# 1. Get all features of class 0 likelihood (P(f | class 0) for each bigram)\n",
    "likelihoods_class_0 = likelihoods[:, 0]\n",
    "\n",
    "# 2. Find the index of the top 5 largest values from largest to smallest\n",
    "top_indices = np.argsort(likelihoods_class_0)[-5:][::-1]\n",
    "\n",
    "# 3. Use the vectorizer to recover the corresponding feature names (bigram) for these indexes.\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_features = [feature_names[i] for i in top_indices]\n",
    "### END OF ANSWER\n",
    "\n",
    "### DO NOT MODIFY\n",
    "print(top_features)\n",
    "pd.DataFrame(top_features).to_csv('./outputs/11c_top_feats.csv')\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecba31",
   "metadata": {},
   "source": [
    "Up to this point, the classifier used bigrams features extracted using CountVectorizer with the wordnet tokenizer. \n",
    "\n",
    "## TASK 1.1d\n",
    "\n",
    "Your task is to improve the naïve Bayes classifier by changing the preprocessing or features only. It is up to you to decide how many changes are needed to improve the classifier -- a single change may be enough to achieve a good result (and maximum marks) and you should only include steps that help performance. Complete the 'preprocess_improved' function below, and run the cell to compute accuracy of the improved classifier on the validation set.     (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4a80f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\lenovo\\anaconda3\\envs\\nlp_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n"
     ]
    }
   ],
   "source": [
    "def preprocess_improved(train_texts):\n",
    "    ### WRITE YOUR ANSWER HERE\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,1), tokenizer=word_tokenize) # Change ngram_range=(1, 1)\n",
    "    X = vectorizer.fit_transform(train_texts).toarray()\n",
    "    num_features = X.shape[1]\n",
    "\n",
    "    X_val = vectorizer.transform(val_texts).toarray()\n",
    "\n",
    "    ### END OF ANSWER\n",
    "\n",
    "    return X, X_val, vectorizer, num_features\n",
    "\n",
    "X, X_val, vectorizer, num_features = preprocess_improved(train_texts)\n",
    "class_priors = compute_class_priors(train_texts, train_labels)\n",
    "likelihoods = compute_feature_likelihoods(X, train_labels)\n",
    "predictions = NB_classify(class_priors, likelihoods, X_val)\n",
    "predictions_nb = np.argmax(predictions, axis=1)\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(predictions_nb).to_csv('./outputs/11d_improved_preds.csv')\n",
    "accuracy_improved = accuracy_score(val_labels, predictions_nb)\n",
    "print(accuracy_improved)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c60bcf",
   "metadata": {},
   "source": [
    "## TASK 1.2\n",
    "\n",
    "Below is an implementation of a neural network classifier that we can apply to the same dataset. However, there are some mistakes in the code and some poor choices in the choice of parameters and architecture. Your task is to fix the errors, make better parameter choices, and improve the model's performance. **Modify the code within the next cell** to improve the neural network classifier, then run it and compute its accuracy using the code in the cell after that.   (8 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8091f8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10 Training Loss: 1.0756\n",
      "Epoch: 1/10 Validation Loss: 0.9855\n",
      "Epoch: 2/10 Training Loss: 0.2348\n",
      "Epoch: 2/10 Validation Loss: 1.0858\n",
      "Epoch: 3/10 Training Loss: 0.0509\n",
      "Epoch: 3/10 Validation Loss: 1.2463\n",
      "Epoch: 4/10 Training Loss: 0.0156\n",
      "Epoch: 4/10 Validation Loss: 1.3464\n",
      "Early stopping triggered.\n",
      "0.55\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY\n",
    "set_seed(my_student_number)\n",
    "###\n",
    "\n",
    "### WRITE YOUR ANSWER HERE: MODIFY THE CODE WITHIN THIS CELL\n",
    " \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"climatebert/distilroberta-base-climate-f\")  \n",
    "\n",
    "sequence_length = 64  ## truncate all docs longer than this. Pad all docs shorter than this. #Too short will truncate most of the key text information.\n",
    "batch_size = 32 ## Small batches lead to unstable training\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],  # Adjust the key based on your dataset structure\n",
    "        padding=\"max_length\",  # Ensures equal sequence lengths\n",
    "        truncation=True,       # Truncates longer sequences\n",
    "        max_length=sequence_length,        # Adjust as needed\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])  # Adjust column names\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "tokenized_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])  # Adjust column names\n",
    "val_loader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class FFTextClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, num_classes, embedding_size=128, hidden_size=128): ## Too small size to learn any meaningful information\n",
    "        super(FFTextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Here we just need to construct the components of our network. We don't need to connect them together yet.\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embedding_size) # embedding layer\n",
    "        self.hidden_layer = nn.Linear(self.embedding_size*sequence_length, hidden_size)  #nn.LSTM(self.embedding_size, hidden_size, bidirectional=True, batch_first=True) # Hidden layer\n",
    "        self.activation = nn.ReLU() # Hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes) # Full connection layer\n",
    "        \n",
    "        \n",
    "    def forward(self, input_words):\n",
    "        # Input dimensions are:  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding_layer(input_words)  # (batch_size, seq_length, embedding_size)\n",
    "\n",
    "        # flatten the sequence of embedding vectors for each document into a single vector.\n",
    "        embedded_words = embedded_words.reshape(embedded_words.shape[0], self.sequence_length*self.embedding_size)  #(embedded_words.shape[0], self.sequence_length*self.embedding_size)  # batch_size, seq_length*embedding_size\n",
    "\n",
    "        z = self.hidden_layer(embedded_words)   # (batch_size, seq_length, hidden_size)\n",
    "        h = self.activation(z)                  # (batch_size, seq_length, hidden_size)\n",
    "\n",
    "        output = self.output_layer(h)                      # (batch_size, num_classes)\n",
    "\n",
    "        # Notice we haven't applied a softmax activation to the output layer -- it's not required by Pytorch's loss function.\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def run_training(self, train_dataloader, dev_dataloader):\n",
    "\n",
    "        # training hyperparameters\n",
    "        num_epochs = 10 ## The code only runs 1 epoch, which is too little. Models may not converge  \n",
    "        learning_rate = 5e-4  ## learning rate for the gradient descent optimizer, related to the step size # Too lowlearning rate, will directly lead to loss of explosion\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss()  # create loss function object\n",
    "        optimizer = optim.Adam(self.parameters(), lr=learning_rate)  # create the optimizer\n",
    "        \n",
    "        dev_losses = []\n",
    "        ## early stopping parameters\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        early_stopping_patience = 3\n",
    "   \n",
    "        for e in range(num_epochs):\n",
    "            # Track performance on the training set as we are learning...\n",
    "            train_losses = []\n",
    "\n",
    "            self.train()  # Put the model in training mode.\n",
    "\n",
    "            for i, batch in enumerate(train_dataloader):\n",
    "                # Iterate over each batch of data\n",
    "\n",
    "                optimizer.zero_grad()  # Reset the optimizer\n",
    "\n",
    "                # Use the model to perform forward inference on the input data.\n",
    "                # This will run the forward() function.\n",
    "                output = self(batch['input_ids'])\n",
    "\n",
    "                # Compute the loss for the current batch of data\n",
    "                batch_loss = loss_fn(output, batch['label'].long()) ## The tag data type must be LongTensor\n",
    "\n",
    "                # Perform back propagation to compute the gradients with respect to each weight\n",
    "                batch_loss.backward()\n",
    "\n",
    "                # Update the weights using the compute gradients\n",
    "                optimizer.step()\n",
    "\n",
    "                # Record the loss from this sample to keep track of progress.\n",
    "                train_losses.append(batch_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "                \"Training Loss: {:.4f}\".format(np.mean(train_losses)))\n",
    "\n",
    "            self.eval()  # Switch model to evaluation mode\n",
    "\n",
    "            dev_losses_epoch = []\n",
    "            \n",
    "            for dev_batch in dev_dataloader:\n",
    "                dev_output = self(dev_batch['input_ids'])\n",
    "                dev_loss = loss_fn(dev_output, dev_batch['label'].long()) ## The tag data type must be LongTensor\n",
    "\n",
    "                # Save the loss on the dev set\n",
    "                dev_losses_epoch.append(dev_loss.item())\n",
    "                        \n",
    "            dev_losses.append(np.mean(dev_losses_epoch))\n",
    "                    \n",
    "            print(\"Epoch: {}/{}\".format((e+1), num_epochs),\n",
    "                \"Validation Loss: {:.4f}\".format(dev_losses[-1]) )\n",
    "\n",
    "            ## Add early stopping\n",
    "            if dev_losses[-1] < best_loss:\n",
    "                best_loss = dev_losses[-1]\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "        \n",
    "        return dev_losses\n",
    "\n",
    "def predict_nn(trained_model, data_loader):\n",
    "\n",
    "    trained_model.eval()\n",
    "\n",
    "    pred_labs = []  # predicted labels to return\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        test_output = trained_model(batch['input_ids'])\n",
    "        predicted_labels = test_output.argmax(1)\n",
    "        pred_labs.extend(predicted_labels.tolist())\n",
    "    \n",
    "    return pred_labs\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab()) ## max(tokenizer.get_vocab().values()) + 1, may missize, error or waste parameter space on the embedding layer.\n",
    "nn_classifier_model = FFTextClassifier(vocab_size, sequence_length, num_classes)\n",
    "dev_losses = nn_classifier_model.run_training(train_loader, val_loader)\n",
    "\n",
    "predictions_nn = predict_nn(nn_classifier_model, val_loader)\n",
    "\n",
    "### END OF ANSWER \n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(predictions_nn).to_csv(\"./outputs/12_nn_preds.csv\")\n",
    "accuracy_nn = accuracy_score(val_labels, predictions_nn)\n",
    "print(accuracy_nn)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ddf4e",
   "metadata": {},
   "source": [
    "We now explore the use of transformers for building a text classifier. First, let's look at how the process a document. We'll chose one at random from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54d2a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY\n",
    "chosen_document = train_texts[np.random.randint(len(train_texts))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45d880",
   "metadata": {},
   "source": [
    "## TASK 1.3a\n",
    "\n",
    "Use the HuggingFace transformers library to load the pretrained BERT model \"prajjwal1/bert-tiny\". Obtain a document embedding for the chosen document given above. Comment your code to explain how it obtains a representation of the document.    (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1266cce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7482838034629822, 1.717582106590271, -4.1115264892578125, -0.24005068838596344, 0.31813621520996094, 0.1779763102531433, -0.08531619608402252, 1.1746528148651123, -1.109370231628418, 0.7128076553344727, 1.4363352060317993, 1.5166279077529907, -0.11133590340614319, 0.3482499420642853, 1.14781653881073, -0.44232845306396484, -0.4152769446372986, -0.9417980313301086, -1.5371623039245605, 0.9611209630966187, -1.5699527263641357, 0.4559035301208496, 1.985022783279419, 0.9156677722930908, 2.5270280838012695, -0.6815090775489807, -0.41216835379600525, -0.6346782445907593, -0.5889894962310791, 0.8110159635543823, 0.16072402894496918, -2.1547656059265137, -0.5654200911521912, -0.4460534453392029, -0.6289471983909607, -0.288046658039093, -0.5391550064086914, 0.9059393405914307, -2.3136513233184814, -0.18432575464248657, -0.24609655141830444, 0.15847896039485931, 0.39705613255500793, -3.1327106952667236, -0.6353223323822021, -2.1962480545043945, -0.1935940384864807, 0.17113542556762695, -1.1163119077682495, 1.3318678140640259, 0.9103641510009766, 1.2458804845809937, -0.8811517953872681, 0.2412213832139969, 0.13286317884922028, 0.191597118973732, -0.9932290315628052, -0.6415081024169922, 0.7123812437057495, 2.7333579063415527, 0.4371885657310486, -0.47053343057632446, -2.3497846126556396, -1.364631175994873, -2.016392946243286, 1.3957773447036743, 0.5076377987861633, 0.6435179710388184, -1.1727356910705566, -1.4572062492370605, -1.4621555805206299, 0.2301412671804428, 0.6470198035240173, -0.04670141637325287, -1.7151036262512207, -0.11927449703216553, 0.7081153988838196, -0.8216554522514343, 0.27839991450309753, -0.5663626790046692, -0.8290435671806335, -0.978728711605072, -0.032441675662994385, 0.8213232159614563, 0.19558706879615784, 0.34633830189704895, 0.9833025932312012, 0.19558696448802948, 0.5069996118545532, -1.8993189334869385, 2.4251744747161865, 0.1796104609966278, 0.09091156721115112, 0.27195635437965393, 0.5935423374176025, 0.30893921852111816, -1.7306523323059082, -0.8407672047615051, 0.5448498725891113, 0.7963560819625854, 0.4963732659816742, -1.2457754611968994, -0.9840366840362549, 0.49448361992836, -0.6222385168075562, 0.43784502148628235, 0.7693958282470703, 0.29026222229003906, 0.2380150407552719, -0.0005137184634804726, -0.05720086395740509, 0.45896345376968384, 0.12015734612941742, 0.04760628938674927, 0.25226879119873047, 1.5865473747253418, 0.530704915523529, 0.816969633102417, -0.3398170471191406, -0.42111724615097046, -0.31603649258613586, 1.6382168531417847, 1.898208737373352, 0.532964289188385, -0.7292382717132568, -0.895464301109314, -1.677587628364563, 1.273329496383667]\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY\n",
    "set_seed(my_student_number)\n",
    "###\n",
    "\n",
    "### WRITE YOUR ANSWER HERE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import numpy as np \n",
    "\n",
    "# Load model and tokenizer (Tiny BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# Select the given document (the one randomly selected in the original notebook code)\n",
    "chosen_document = train_texts[np.random.randint(len(train_texts))]\n",
    "\n",
    "# Use tokenizer to encode documents for input_ids and attention_mask\n",
    "inputs = tokenizer(\n",
    "    chosen_document,\n",
    "    return_tensors=\"pt\",        # Return to PyTorch tensors\n",
    "    truncation=True,            # Truncate long text\n",
    "    padding=\"max_length\",       # Fill to maximum length (default 512)\n",
    "    max_length=128              # Custom length prevents overflow\n",
    ")\n",
    "\n",
    "# Input model gets output\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# outputs.last_hidden_state's shape is (1, seq_len, hidden_dim)\n",
    "# We take the output of the first token ([CLS] token) as the embedding representation of the entire document\n",
    "cls_embedding = outputs.last_hidden_state[:, 0, :]  # shape: (1, hidden_dim)\n",
    "\n",
    "doc_emb = []  # use this variable to store the document embedding\n",
    "\n",
    "# Append the values from tensor to the doc_emb list\n",
    "doc_emb.extend(cls_embedding.squeeze().tolist())\n",
    "### END OF ANSWER\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(doc_emb).to_csv('./outputs/13a_sen_emb.csv')\n",
    "print(doc_emb)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93e1d9",
   "metadata": {},
   "source": [
    "## TASK 1.3b\n",
    "\n",
    "Using the same document embeddings method as the previous task, find the most similar document to the 'chosen_document' from within the validation set (from the 'val_texts' object). Use a standard similarity metric that considers the direction but not the magnitude of the embedding vectors. Use the same model as in task 1.8.  (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "19d97094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLES OF RISKS Resource scarcity, coupled with increasing demand, could affect production, availability, quality and cost of raw materials. Increased frequency of extreme weather events, from floods to droughts, could cause disruption in our supply chain and impact the sourcing of raw materials, as well as the production and distribution of finished goods. Increased regulation and more stringent environmental standards could impact our business by affecting production costs and flexibility of operations. Our industry is sustained by many agricultural and manufacturing communities around the world. Failure to support them in preserving key skills and building more sustainable livelihoods could cause social, economic and operational challenges, ranging from community tensions and disruption to production, to a reduced talent pool.\n",
      "Climate change presents an evolving set of risks and opportunities for Coles, and has the potential to contribute to and increase the exposure of other material risks. These include increased frequency/intensity of extreme weather events and chronic climate changes which can disrupt our operations and compromise the safety of our team members, customers, supply chain and the food we sell; changes to government policy, law and regulation, which can result in increased costs to operate and potential for litigation; and failure to meet expectations of stakeholders resulting in reputational damage.\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY\n",
    "set_seed(my_student_number)\n",
    "###\n",
    "\n",
    "### WRITE YOUR ANSWER HERE\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "most_similar_doc = \"\"  # use this variable to store the most similar document\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# Gets the embedding of the chosen_document (same as 1.3a)\n",
    "inputs = tokenizer(\n",
    "    chosen_document,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "chosen_emb = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "chosen_emb = chosen_emb.squeeze().numpy().reshape(1, -1)  # shape: (1, hidden_size)\n",
    "\n",
    "# Computes embedding and similarity for all validation set documents\n",
    "val_embeddings = []\n",
    "for text in val_texts:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    emb = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "    val_embeddings.append(emb)\n",
    "\n",
    "val_embeddings = np.stack(val_embeddings)  # shape: (N, hidden_size)\n",
    "\n",
    "# Use cosine similarity\n",
    "similarities = cosine_similarity(chosen_emb, val_embeddings)[0]  # shape: (N,)\n",
    "most_similar_index = np.argmax(similarities)\n",
    "most_similar_doc = val_texts[most_similar_index]\n",
    "### END OF ANSWER\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame([chosen_document, most_similar_doc]).to_csv(\"./outputs/13b_most_similar.csv\")\n",
    "print(chosen_document)\n",
    "print(most_similar_doc)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89641896",
   "metadata": {},
   "source": [
    "## TASK 1.3c\n",
    "\n",
    "Implement a classifier based on the same pretrained transformer model, \"prajjwal1/bert-tiny\". Evaluate your model's performance on the validation set. Use an 'auto class' from HuggingFace to build your classifier (see https://huggingface.co/docs/transformers/model_doc/auto).   (6 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2e6fd664",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Train Loss: 1.0787\n",
      "Validation Loss: 1.0476\n",
      "Epoch 2 completed. Train Loss: 1.0117\n",
      "Validation Loss: 0.9841\n",
      "Epoch 3 completed. Train Loss: 0.9382\n",
      "Validation Loss: 0.9235\n",
      "Epoch 4 completed. Train Loss: 0.8632\n",
      "Validation Loss: 0.8708\n",
      "Epoch 5 completed. Train Loss: 0.7843\n",
      "Validation Loss: 0.7993\n",
      "Epoch 6 completed. Train Loss: 0.7080\n",
      "Validation Loss: 0.7452\n",
      "Epoch 7 completed. Train Loss: 0.6360\n",
      "Validation Loss: 0.7061\n",
      "Epoch 8 completed. Train Loss: 0.5731\n",
      "Validation Loss: 0.6802\n",
      "Epoch 9 completed. Train Loss: 0.5160\n",
      "Validation Loss: 0.6527\n",
      "Epoch 10 completed. Train Loss: 0.4638\n",
      "Validation Loss: 0.6475\n",
      "Epoch 11 completed. Train Loss: 0.4165\n",
      "Validation Loss: 0.6290\n",
      "Epoch 12 completed. Train Loss: 0.3725\n",
      "Validation Loss: 0.6063\n",
      "Epoch 13 completed. Train Loss: 0.3304\n",
      "Validation Loss: 0.6086\n",
      "Epoch 14 completed. Train Loss: 0.2898\n",
      "Validation Loss: 0.5897\n",
      "Epoch 15 completed. Train Loss: 0.2536\n",
      "Validation Loss: 0.5885\n",
      "Epoch 16 completed. Train Loss: 0.2214\n",
      "Validation Loss: 0.6022\n",
      "Epoch 17 completed. Train Loss: 0.1925\n",
      "Validation Loss: 0.6059\n",
      "Early stopping triggered.\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY\n",
    "set_seed(my_student_number)\n",
    "###\n",
    "\n",
    "### WRITE YOUR ANSWER HERE\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "predictions_bert = []  # use this variable to store the predicted labels for the validation set\n",
    "\n",
    "# Load tokenizer and classification model (num_labels=3, since it's three-class)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=3)\n",
    "\n",
    "# Define the datasets class\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Build the Dataset and DataLoader\n",
    "train_data = ClimateDataset(train_texts, train_labels)\n",
    "val_data = ClimateDataset(val_texts, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "# Model training\n",
    "from torch import nn, optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "num_epochs = 20\n",
    "early_stopping_patience = 2\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "for epoch in range(num_epochs): \n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} completed. Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Model evaluation(Prediction validation)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=1).cpu().tolist()\n",
    "        predictions_bert.extend(preds)\n",
    "\n",
    "### END OF ANSWER\n",
    "\n",
    "### DO NOT MODIFY\n",
    "pd.DataFrame(predictions_bert).to_csv('./outputs/13c_bert_preds.csv')\n",
    "accuracy_tinybert = accuracy_score(val_dataset[\"label\"], predictions_bert)\n",
    "print(accuracy_tinybert)\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5038097-70f7-47bf-b451-f620530945ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37411a09-06d1-4f43-a1f6-f0f954eaffdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP Env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
